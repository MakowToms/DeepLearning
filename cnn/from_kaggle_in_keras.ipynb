{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "from_kaggle_in_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxMQE4Vmvl5koRcoFmLu6a"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGrapZ2qY-YE"
      },
      "source": [
        "The CIFAR first steps. The following notebook is based on code from kaggle platform:\n",
        "https://www.kaggle.com/roblexnana/cifar10-with-cnn-for-beginer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFDGGg5xYc5O"
      },
      "source": [
        "First import all necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGDK67tZXTXZ"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wacLdYwYp7J"
      },
      "source": [
        "Set global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0VKF1xiYjoW"
      },
      "source": [
        "batch_size = 32  # The default batch size of keras.\n",
        "num_classes = 10  # Number of class for the dataset\n",
        "epochs = 100\n",
        "data_augmentation = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq7c2BjgYvb0",
        "outputId": "0b103184-18ce-44fd-ed1d-0b7489f6cc99"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 1)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AazejXObY44o"
      },
      "source": [
        "# Normalize the data. Before we need to connvert data type to float for computation.\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices. This is called one hot encoding.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5VKw2nNaJil",
        "outputId": "5e191dd0-2979-4731-ce0b-264199f0306b"
      },
      "source": [
        "#define the model - convolution network\n",
        "model = Sequential()\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# FLATTERN => DENSE => RELU => DROPOUT\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "# a softmax classifier\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,250,858\n",
            "Trainable params: 1,250,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPj9xXLCanT7"
      },
      "source": [
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rukheCMybQA8",
        "outputId": "a1d76ff8-437b-4bc6-cbaa-01f52859d74b"
      },
      "source": [
        "history = None  # For recording the history of trainning process.\n",
        "np.random.seed(123)\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                    batch_size=batch_size),\n",
        "                                    epochs=epochs,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    workers=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not using data augmentation.\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 239s 153ms/step - loss: 1.8111 - accuracy: 0.3394 - val_loss: 1.5478 - val_accuracy: 0.4365\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 237s 152ms/step - loss: 1.5161 - accuracy: 0.4484 - val_loss: 1.3675 - val_accuracy: 0.5078\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 1.3788 - accuracy: 0.5022 - val_loss: 1.2457 - val_accuracy: 0.5578\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 237s 152ms/step - loss: 1.2801 - accuracy: 0.5415 - val_loss: 1.2521 - val_accuracy: 0.5580\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 233s 149ms/step - loss: 1.2018 - accuracy: 0.5750 - val_loss: 1.1858 - val_accuracy: 0.5896\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 232s 148ms/step - loss: 1.1328 - accuracy: 0.5990 - val_loss: 1.0881 - val_accuracy: 0.6134\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 232s 148ms/step - loss: 1.0768 - accuracy: 0.6207 - val_loss: 0.9901 - val_accuracy: 0.6562\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 232s 148ms/step - loss: 1.0288 - accuracy: 0.6366 - val_loss: 0.9984 - val_accuracy: 0.6460\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 231s 148ms/step - loss: 0.9904 - accuracy: 0.6521 - val_loss: 0.9335 - val_accuracy: 0.6781\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 231s 147ms/step - loss: 0.9542 - accuracy: 0.6660 - val_loss: 0.8831 - val_accuracy: 0.6972\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 231s 148ms/step - loss: 0.9226 - accuracy: 0.6791 - val_loss: 0.8832 - val_accuracy: 0.6928\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 231s 148ms/step - loss: 0.8973 - accuracy: 0.6855 - val_loss: 0.8540 - val_accuracy: 0.7084\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 231s 148ms/step - loss: 0.8723 - accuracy: 0.6948 - val_loss: 0.8581 - val_accuracy: 0.7060\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 232s 148ms/step - loss: 0.8521 - accuracy: 0.7023 - val_loss: 0.8065 - val_accuracy: 0.7221\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.8302 - accuracy: 0.7110 - val_loss: 0.7936 - val_accuracy: 0.7236\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.8177 - accuracy: 0.7142 - val_loss: 0.7840 - val_accuracy: 0.7282\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.8023 - accuracy: 0.7221 - val_loss: 0.7913 - val_accuracy: 0.7257\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.7866 - accuracy: 0.7278 - val_loss: 0.7887 - val_accuracy: 0.7287\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.7743 - accuracy: 0.7315 - val_loss: 0.7699 - val_accuracy: 0.7365\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.7654 - accuracy: 0.7361 - val_loss: 0.7384 - val_accuracy: 0.7472\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.7530 - accuracy: 0.7393 - val_loss: 0.7496 - val_accuracy: 0.7428\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 236s 151ms/step - loss: 0.7431 - accuracy: 0.7439 - val_loss: 0.7436 - val_accuracy: 0.7459\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 237s 151ms/step - loss: 0.7380 - accuracy: 0.7475 - val_loss: 0.7246 - val_accuracy: 0.7538\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 237s 151ms/step - loss: 0.7302 - accuracy: 0.7514 - val_loss: 0.7416 - val_accuracy: 0.7475\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 237s 151ms/step - loss: 0.7236 - accuracy: 0.7523 - val_loss: 0.7237 - val_accuracy: 0.7521\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.7153 - accuracy: 0.7573 - val_loss: 0.7214 - val_accuracy: 0.7646\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.7079 - accuracy: 0.7591 - val_loss: 0.8039 - val_accuracy: 0.7342\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 237s 152ms/step - loss: 0.7074 - accuracy: 0.7583 - val_loss: 0.7063 - val_accuracy: 0.7633\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 237s 152ms/step - loss: 0.7015 - accuracy: 0.7612 - val_loss: 0.7493 - val_accuracy: 0.7485\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 237s 152ms/step - loss: 0.6992 - accuracy: 0.7651 - val_loss: 0.7016 - val_accuracy: 0.7666\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.6974 - accuracy: 0.7645 - val_loss: 0.7046 - val_accuracy: 0.7652\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.6913 - accuracy: 0.7667 - val_loss: 0.6935 - val_accuracy: 0.7667\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.6866 - accuracy: 0.7675 - val_loss: 0.6853 - val_accuracy: 0.7718\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.6810 - accuracy: 0.7704 - val_loss: 0.7091 - val_accuracy: 0.7666\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.6801 - accuracy: 0.7699 - val_loss: 0.6915 - val_accuracy: 0.7730\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.6755 - accuracy: 0.7730 - val_loss: 0.7115 - val_accuracy: 0.7782\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.6790 - accuracy: 0.7734"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikHHasbYb86A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "6249e4bd-f32a-420f-e20e-550af623dc0c"
      },
      "source": [
        "def plotmodelhistory(history): \n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(history.history['accuracy']) \n",
        "    axs[0].plot(history.history['val_accuracy']) \n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy') \n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(history.history['loss']) \n",
        "    axs[1].plot(history.history['val_loss']) \n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss') \n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "\n",
        "plotmodelhistory(history)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d35239ce4e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# list all data in history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mplotmodelhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM-DWrcdcIYU"
      },
      "source": [
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n",
        "# make prediction.\n",
        "pred = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ8WiybqbgzC"
      },
      "source": [
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}